{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86b9dd0d-cefe-43ce-aff4-c8ab778d42c5",
   "metadata": {},
   "source": [
    "# SVM classification for PTM (whole brain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269bee70-b127-4f27-9e8e-71e969b01a77",
   "metadata": {},
   "source": [
    "## Yifan Gao 2025-04-09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "589bccfd-7333-49ab-bab2-0c3c71e80e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "from scipy import stats\n",
    "from scipy.io import loadmat\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import decimate\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.datasets import load_iris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226432bd-6ace-4810-936b-12b042698bf9",
   "metadata": {},
   "source": [
    "# Organize individual data sets and run SVM on individual early and late blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "8b10155e-46a9-43a6-bd20-0510d0b564da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory exists.\n",
      "Loading data: PTM_0020\n",
      "(53, 340, 32)\n",
      "(63, 340, 32)\n",
      "Loading data: PTM_0021\n",
      "(63, 340, 32)\n",
      "(64, 340, 32)\n",
      "Loading data: PTM_0022\n",
      "(63, 340, 32)\n",
      "(64, 340, 32)\n",
      "Loading data: PTM_0023\n",
      "(64, 340, 32)\n",
      "(64, 340, 32)\n",
      "Loading data: PTM_0024\n",
      "(51, 340, 32)\n",
      "(28, 340, 32)\n",
      "Loading data: PTM_0025\n",
      "(64, 340, 32)\n",
      "(64, 340, 32)\n",
      "Loading data: PTM_0026\n",
      "(63, 340, 32)\n",
      "(62, 340, 32)\n",
      "Loading data: PTM_0027\n",
      "(63, 340, 32)\n",
      "(64, 340, 32)\n",
      "Loading data: PTM_0028\n",
      "(64, 340, 32)\n",
      "(63, 340, 32)\n",
      "Loading data: PTM_0029\n",
      "(64, 340, 32)\n",
      "(64, 340, 32)\n",
      "Loading data: PTM_0030\n",
      "(54, 340, 32)\n",
      "(63, 340, 32)\n",
      "Loading data: PTM_0031\n",
      "(64, 340, 32)\n",
      "(64, 340, 32)\n",
      "Loading data: PTM_0032\n",
      "(60, 340, 32)\n",
      "(54, 340, 32)\n",
      "Loading data: PTM_0033\n",
      "(64, 340, 32)\n",
      "(64, 340, 32)\n",
      "Loading data: PTM_0034\n",
      "(63, 340, 32)\n",
      "(63, 340, 32)\n",
      "Loading data: PTM_0035\n",
      "(62, 340, 32)\n",
      "(63, 340, 32)\n",
      "Loading data: PTM_0036\n",
      "(61, 340, 32)\n",
      "(60, 340, 32)\n",
      "Loading data: PTM_0037\n",
      "(63, 340, 32)\n",
      "(62, 340, 32)\n",
      "Loading data: PTM_0038\n",
      "(61, 340, 32)\n",
      "(58, 340, 32)\n",
      "Loading data: PTM_0040\n",
      "(60, 340, 32)\n",
      "(63, 340, 32)\n",
      "Loading data: PTM_0041\n",
      "(63, 340, 32)\n",
      "(63, 340, 32)\n",
      "Loading data: PTM_0042\n",
      "(62, 340, 32)\n",
      "(62, 340, 32)\n",
      "Loading data: PTM_0043\n",
      "(62, 340, 32)\n",
      "(63, 340, 32)\n",
      "Loading data: PTM_0044\n",
      "(63, 340, 32)\n",
      "(64, 340, 32)\n",
      "Loading data: PTM_0045\n",
      "(52, 340, 32)\n",
      "(62, 340, 32)\n",
      "Loading data: PTM_0046\n",
      "(63, 340, 32)\n",
      "(64, 340, 32)\n",
      "Loading data: PTM_0047\n",
      "(62, 340, 32)\n",
      "(64, 340, 32)\n",
      "Loading data: PTM_0048\n",
      "(63, 340, 32)\n",
      "(64, 340, 32)\n",
      "Loading data: PTM_0049\n",
      "(62, 340, 32)\n",
      "(63, 340, 32)\n",
      "Loading data: PTM_0050\n",
      "(64, 340, 32)\n",
      "(64, 340, 32)\n",
      "Loading data: PTM_0051\n",
      "(64, 340, 32)\n",
      "(64, 340, 32)\n",
      "Loading data: PTM_0052\n",
      "(63, 340, 32)\n",
      "(64, 340, 32)\n",
      "Loading data: PTM_0053\n",
      "(59, 340, 32)\n",
      "(60, 340, 32)\n",
      "Loading data: PTM_0054\n",
      "(63, 340, 32)\n",
      "(64, 340, 32)\n",
      "Loading data: PTM_0055\n",
      "(64, 340, 32)\n",
      "(61, 340, 32)\n",
      "Loading data: PTM_0057\n",
      "(64, 340, 32)\n",
      "(55, 340, 32)\n",
      "Loading data: PTM_0058\n",
      "(55, 340, 32)\n",
      "(63, 340, 32)\n",
      "Loading data: PTM_0059\n",
      "(52, 340, 32)\n",
      "(38, 340, 32)\n",
      "Loading data: PTM_0060\n",
      "(63, 340, 32)\n",
      "(64, 340, 32)\n",
      "Loading data: PTM_0061\n",
      "(63, 340, 32)\n",
      "(64, 340, 32)\n",
      "Loading data: PTM_0062\n",
      "(64, 340, 32)\n",
      "(63, 340, 32)\n",
      "Loading data: PTM_0063\n",
      "(62, 340, 32)\n",
      "(64, 340, 32)\n",
      "Loading data: PTM_0064\n",
      "(63, 340, 32)\n",
      "(62, 340, 32)\n",
      "Loading data: PTM_0065\n",
      "(64, 340, 32)\n",
      "(63, 340, 32)\n",
      "Loading data: PTM_0066\n",
      "(64, 340, 32)\n",
      "(63, 340, 32)\n",
      "Loading data: PTM_0067\n",
      "(64, 340, 32)\n",
      "(62, 340, 32)\n",
      "Loading data: PTM_0068\n",
      "(64, 340, 32)\n",
      "(64, 340, 32)\n",
      "Loading data: PTM_0069\n",
      "(64, 340, 32)\n",
      "(64, 340, 32)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_early_all_200Hz' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[289], line 171\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# append single subject accuracy scores together\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     accuracy_late_all\u001b[38;5;241m.\u001b[39mappend(accuracy_late)\n\u001b[0;32m--> 171\u001b[0m accuracy_early_all \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(accuracy_early_all_200Hz)\n\u001b[1;32m    172\u001b[0m accuracy_late_all \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(accuracy_late_all_200Hz)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy_early_all_200Hz' is not defined"
     ]
    }
   ],
   "source": [
    "pathname_read = r'/Users/ygao/Desktop/Yifan/PTM/study3/data/EEG/exportfiles_v3'\n",
    "\n",
    "if os.path.exists(pathname_read):\n",
    "    print(\"The directory exists.\")\n",
    "else:\n",
    "    print(\"The directory does not exist.\")\n",
    "\n",
    "#subjs = ['0020', '0021']\n",
    "subjs = ['0020', '0021', '0022', '0023', '0024', '0025', '0026', '0027', '0028', '0029',\n",
    "        '0030', '0031', '0032', '0033', '0034', '0035', '0036', '0037', '0038',\n",
    "        '0040', '0041', '0042', '0043', '0044', '0045', '0046', '0047', '0048', '0049',\n",
    "        '0050', '0051', '0052', '0053', '0054', '0055', '0057', '0058', '0059',\n",
    "        '0060', '0061', '0062', '0063', '0064', '0065', '0066', '0067', '0068', '0069']\n",
    "#0056 didn't have EEG\n",
    "#0039 had insufficient segments for k-fold cross-validation\n",
    "n_subjs = len(subjs)\n",
    "\n",
    "accuracy_early_all = []\n",
    "accuracy_late_all = []\n",
    "\n",
    "for subj in subjs: \n",
    "    print(\"Loading data: PTM_{}\".format(subj))\n",
    "\n",
    "    matA1 = loadmat(pathname_read + r'/PTM_' + subj + r'_A_blk1.mat')\n",
    "    matA2 = loadmat(pathname_read + r'/PTM_' + subj + r'_A_blk3.mat')\n",
    "    matB1 = loadmat(pathname_read + r'/PTM_' + subj + r'_B_blk1.mat')\n",
    "    matB2 = loadmat(pathname_read + r'/PTM_' + subj + r'_B_blk3.mat')\n",
    "\n",
    "    # sanity check for matrix size\n",
    "    #print(matA1['Fz'].shape)\n",
    "    #print(matA2['Fz'].shape)\n",
    "    #print(matB1['Fz'].shape)\n",
    "    #print(matB2['Fz'].shape)\n",
    "\n",
    "    # merge all channels into third dimension\n",
    "    channel_names = ['Fp1', 'Fz', 'F3', 'LH', 'F7', 'FC5', 'FC1', 'C3', 'LM', 'CP5', 'CP1', \n",
    "                 'Pz', 'P3', 'P7', 'O1', 'POz', 'FCz', 'VEOG', 'P4', 'P8', 'RM', 'CP6', \n",
    "                 'CP2', 'Cz', 'C4', 'O2', 'F8', 'FC6', 'FC2', 'F4', 'RH', 'Fp2']\n",
    "\n",
    "    mat_names = ['matA1', 'matA2', 'matB1', 'matB2']\n",
    "    \n",
    "    # stack all channels and make 3D matrices\n",
    "    for mat_name in mat_names:\n",
    "        mat_data = globals()[mat_name] #loop through all mat types\n",
    "        \n",
    "        chan_data = []\n",
    "        for chan in channel_names:\n",
    "            if chan in mat_data:\n",
    "                chan_data.append(mat_data[chan])\n",
    "        \n",
    "        #stack the 2d matrix of all channels\n",
    "        mat_3d = np.stack(chan_data, axis=-1) \n",
    "\n",
    "        # downsample data to 200Hz\n",
    "        downsampled_mat_3d = decimate(mat_3d, 5, axis=1)\n",
    "        #print(downsampled_mat_3d.shape)\n",
    "    \n",
    "        globals()[f\"{mat_name}_3d\"] = downsampled_mat_3d\n",
    "\n",
    "    # create corresponding label matrices\n",
    "    matA1_label = np.ones(matA1_3d.shape[0])\n",
    "    matA2_label = np.ones(matA2_3d.shape[0])\n",
    "    matB1_label = np.zeros(matB1_3d.shape[0])\n",
    "    matB2_label = np.zeros(matB2_3d.shape[0])\n",
    "\n",
    "    # append the A and B matrices together into early and late x and y matrices\n",
    "    early_x = np.concatenate((matA1_3d, matB1_3d), axis=0)\n",
    "    early_y = np.append(matA1_label,matB1_label)\n",
    "    print(early_x.shape)\n",
    "    #print(early_y.shape)\n",
    "    #print(early_y)\n",
    "    \n",
    "    late_x = np.concatenate((matA2_3d, matB2_3d), axis=0)\n",
    "    late_y = np.append(matA2_label,matB2_label)\n",
    "    print(late_x.shape)\n",
    "    #print(late_y.shape)\n",
    "    #print(late_y)\n",
    "\n",
    "    # run SVM for whole brain across each time point\n",
    "    accuracy_early = []\n",
    "    accuracy_late = []\n",
    "    \n",
    "    # Run the model for early block\n",
    "    for i in range(0, early_x.shape[1]):\n",
    "        #print(early_x[:,1,:].shape)\n",
    "        early_x_ti = early_x[:,i,:]\n",
    "        #print(early_x_ti.shape)\n",
    "    \n",
    "        X = early_x_ti\n",
    "        y = early_y\n",
    "        \n",
    "        # Create a KFold object with k=5\n",
    "        kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=None)\n",
    "        \n",
    "        # Initialize an empty list to store accuracy scores\n",
    "        acc_ti = []\n",
    "        \n",
    "        # Iterate through the folds\n",
    "        for train_index, test_index in kf.split(X,y):\n",
    "            # Split the data into training and testing sets\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "            clf = SVC(kernel='linear', class_weight='balanced')\n",
    "            clf.fit(X_train, y_train)\n",
    "            #print(y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            \n",
    "            # Calculate accuracy and append to the list\n",
    "            acc_fold = accuracy_score(y_test, y_pred)\n",
    "            acc_ti.append(acc_fold)\n",
    "        \n",
    "        # Calculate the average accuracy across all folds\n",
    "        avg_acc_ti = np.mean(acc_ti)\n",
    "        #print(avg_acc_ti)\n",
    "        \n",
    "        # Print the accuracy scores for each fold and the average accuracy\n",
    "        #print(\"Accuracy scores for each fold:\", accuracy_scores)\n",
    "        #print(\"Average accuracy:\", average_accuracy)\n",
    "    \n",
    "        # Save average accuracy scores for a single time point\n",
    "        accuracy_early.append(avg_acc_ti)\n",
    "\n",
    "    # append single subject accuracy scores together\n",
    "    accuracy_early_all.append(accuracy_early)\n",
    "    \n",
    "    # Run the model for late block\n",
    "    for i in range(0, late_x.shape[1]):\n",
    "        #print(late_x[:,1,:].shape)\n",
    "        late_x_ti = late_x[:,i,:]\n",
    "        #print(late_x_ti.shape)\n",
    "    \n",
    "        X = late_x_ti\n",
    "        y = late_y\n",
    "        \n",
    "        # Create a KFold object with k=5\n",
    "        kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=None)\n",
    "        \n",
    "        # Initialize an empty list to store accuracy scores\n",
    "        acc_ti = []\n",
    "        \n",
    "        # Iterate through the folds\n",
    "        for train_index, test_index in kf.split(X,y):\n",
    "            # Split the data into training and testing sets\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "            clf = SVC(kernel='linear', class_weight='balanced')\n",
    "            clf.fit(X_train, y_train)\n",
    "            #print(y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            \n",
    "            # Calculate accuracy and append to the list\n",
    "            acc_fold = accuracy_score(y_test, y_pred)\n",
    "            acc_ti.append(acc_fold)\n",
    "        \n",
    "        # Calculate the average accuracy across all folds\n",
    "        avg_acc_ti = np.mean(acc_ti)\n",
    "        #print(avg_acc_ti)\n",
    "        \n",
    "        # Print the accuracy scores for each fold and the average accuracy\n",
    "        #print(\"Accuracy scores for each fold:\", accuracy_scores)\n",
    "        #print(\"Average accuracy:\", average_accuracy)\n",
    "    \n",
    "        # Save average accuracy scores for a single time point\n",
    "        accuracy_late.append(avg_acc_ti)\n",
    "\n",
    "    # append single subject accuracy scores together\n",
    "    accuracy_late_all.append(accuracy_late)\n",
    "\n",
    "accuracy_early_all = np.array(accuracy_early_all)\n",
    "accuracy_late_all = np.array(accuracy_late_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2d619d-27ad-4459-818e-33d8f08bdb78",
   "metadata": {},
   "source": [
    "## Save the accuracy scores for all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "c53141e4-e879-44ac-b2d2-8edcc217172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"accuracy_early_all_200Hz.csv\", accuracy_early_all, delimiter=\",\")\n",
    "np.savetxt(\"accuracy_late_all_200Hz.csv\", accuracy_late_all, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c42d54-f598-4d56-98dc-a01b1c6ca29d",
   "metadata": {},
   "source": [
    "# Try neural network classification (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "896501da-b100-455b-85ef-09ded67e8102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Early Block ===\n",
      "Accuracy on held-out data: 92.93%\n",
      "Confusion Matrix:\n",
      "[[4 1]\n",
      " [3 3]]\n",
      "=== Late Block ===\n",
      "Accuracy on held-out data: 92.93%\n",
      "Confusion Matrix:\n",
      "[[3 3]\n",
      " [2 5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Early block\n",
    "# Split data into training and testing (80% train, 20% test)\n",
    "early_x_train, early_x_test, early_y_train, early_y_test = train_test_split(\n",
    "    early_x_2d, early_y, test_size=0.2, stratify=early_y)\n",
    "# take out random variable\n",
    "# 20% test\n",
    "# features = channels\n",
    "# samples = trials\n",
    "\n",
    "# Train a neural network classifier on the training data\n",
    "# The network has one hidden layer with 10 neurons\n",
    "nn_early = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000)\n",
    "nn_early.fit(early_x_train, early_y_train)\n",
    "\n",
    "# Predict\n",
    "early_y_pred = nn_early.predict(early_x_test)\n",
    "\n",
    "# Evaluate\n",
    "acc_early = accuracy_score(early_y_test, early_y_pred)\n",
    "cm_early = confusion_matrix(early_y_test, early_y_pred)\n",
    "\n",
    "print(\"=== Early Block ===\")\n",
    "print(\"Accuracy on held-out data: {:.2f}%\".format(accuracy * 100))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_early)\n",
    "\n",
    "# Late block\n",
    "# Split data into training and testing (80% train, 20% test)\n",
    "late_x_train, late_x_test, late_y_train, late_y_test = train_test_split(\n",
    "    late_x_2d, late_y, test_size=0.2, stratify=late_y)\n",
    "# take out random variable\n",
    "# 20% test\n",
    "# features = channels\n",
    "# samples = trials\n",
    "\n",
    "# Train a neural network classifier on the training data\n",
    "# The network has one hidden layer with 10 neurons\n",
    "nn_late = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000)\n",
    "nn_late.fit(late_x_train, late_y_train)\n",
    "\n",
    "# Predict\n",
    "late_y_pred = nn_late.predict(late_x_test)\n",
    "\n",
    "# Evaluate\n",
    "acc_late = accuracy_score(late_y_test, late_y_pred)\n",
    "cm_late = confusion_matrix(late_y_test, late_y_pred)\n",
    "\n",
    "print(\"=== Late Block ===\")\n",
    "print(\"Accuracy on held-out data: {:.2f}%\".format(accuracy * 100))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_late)\n",
    "\n",
    "# why does the confusion matrix look so weird?\n",
    "\n",
    "\n",
    "pathname_read = r'/Users/ygao/Desktop/Yifan/PTM/study3/data/EEG/exportfiles_v3'\n",
    "\n",
    "if os.path.exists(pathname_read):\n",
    "    print(\"The directory exists.\")\n",
    "else:\n",
    "    print(\"The directory does not exist.\")\n",
    "\n",
    "#subjs = ['0020', '0021']\n",
    "subjs = ['0020', '0021', '0022', '0023', '0024', '0025', '0026', '0027', '0028', '0029',\n",
    "        '0030', '0031', '0032', '0033', '0034', '0035', '0036', '0037', '0038',\n",
    "        '0040', '0041', '0042', '0043', '0044', '0045', '0046', '0047', '0048', '0049',\n",
    "        '0050', '0051', '0052', '0053', '0054', '0055', '0057', '0058', '0059',\n",
    "        '0060', '0061', '0062', '0063', '0064', '0065', '0066', '0067', '0068', '0069']\n",
    "#0056 didn't have EEG\n",
    "#0039 had insufficient segments for k-fold cross-validation\n",
    "n_subjs = len(subjs)\n",
    "\n",
    "accuracy_early_all = []\n",
    "accuracy_late_all = []\n",
    "\n",
    "for subj in subjs: \n",
    "    print(\"Loading data: PTM_{}\".format(subj))\n",
    "\n",
    "    matA1 = loadmat(pathname_read + r'/PTM_' + subj + r'_A_blk1.mat')\n",
    "    matA2 = loadmat(pathname_read + r'/PTM_' + subj + r'_A_blk3.mat')\n",
    "    matB1 = loadmat(pathname_read + r'/PTM_' + subj + r'_B_blk1.mat')\n",
    "    matB2 = loadmat(pathname_read + r'/PTM_' + subj + r'_B_blk3.mat')\n",
    "\n",
    "    # sanity check for matrix size\n",
    "    #print(matA1['Fz'].shape)\n",
    "    #print(matA2['Fz'].shape)\n",
    "    #print(matB1['Fz'].shape)\n",
    "    #print(matB2['Fz'].shape)\n",
    "\n",
    "    # merge all channels into third dimension\n",
    "    channel_names = ['Fp1', 'Fz', 'F3', 'LH', 'F7', 'FC5', 'FC1', 'C3', 'LM', 'CP5', 'CP1', \n",
    "                 'Pz', 'P3', 'P7', 'O1', 'POz', 'FCz', 'VEOG', 'P4', 'P8', 'RM', 'CP6', \n",
    "                 'CP2', 'Cz', 'C4', 'O2', 'F8', 'FC6', 'FC2', 'F4', 'RH', 'Fp2']\n",
    "\n",
    "    mat_names = ['matA1', 'matA2', 'matB1', 'matB2']\n",
    "    \n",
    "    # stack all channels and make 3D matrices\n",
    "    for mat_name in mat_names:\n",
    "        mat_data = globals()[mat_name] #loop through all mat types\n",
    "        \n",
    "        chan_data = []\n",
    "        for chan in channel_names:\n",
    "            if chan in mat_data:\n",
    "                chan_data.append(mat_data[chan])\n",
    "    \n",
    "        mat_3d = np.stack(chan_data, axis=-1) #stack the 2d matrix of all channels\n",
    "    \n",
    "        globals()[f\"{mat_name}_3d\"] = mat_3d\n",
    "        #print(f\"{mat_name}_3d shape:\", mat_3d.shape)\n",
    "    \n",
    "    # create corresponding label matrices\n",
    "    matA1_label = np.ones(matA1_3d.shape[0])\n",
    "    matA2_label = np.ones(matA2_3d.shape[0])\n",
    "    matB1_label = np.zeros(matB1_3d.shape[0])\n",
    "    matB2_label = np.zeros(matB2_3d.shape[0])\n",
    "\n",
    "    # append the A and B matrices together into early and late x and y matrices\n",
    "    early_x = np.concatenate((matA1_3d, matB1_3d), axis=0)\n",
    "    early_y = np.append(matA1_label,matB1_label)\n",
    "    #print(early_x.shape)\n",
    "    #print(early_y.shape)\n",
    "    #print(early_y)\n",
    "    \n",
    "    late_x = np.concatenate((matA2_3d, matB2_3d), axis=0)\n",
    "    late_y = np.append(matA2_label,matB2_label)\n",
    "    #print(late_x.shape)\n",
    "    #print(late_y.shape)\n",
    "    #print(late_y)\n",
    "\n",
    "    # run SVM for whole brain across each time point\n",
    "    accuracy_early = []\n",
    "    accuracy_late = []\n",
    "    \n",
    "    # Run the model for early block\n",
    "    for i in range(0, early_x.shape[1]):\n",
    "        #print(early_x[:,1,:].shape)\n",
    "        early_x_ti = early_x[:,i,:]\n",
    "        #print(early_x_ti.shape)\n",
    "    \n",
    "        X = early_x_ti\n",
    "        y = early_y\n",
    "        \n",
    "        # Create a KFold object with k=5\n",
    "        kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=None)\n",
    "        \n",
    "        # Initialize an empty list to store accuracy scores\n",
    "        acc_ti = []\n",
    "        \n",
    "        # Iterate through the folds\n",
    "        for train_index, test_index in kf.split(X,y):\n",
    "            # Split the data into training and testing sets\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            # The network has one hidden layer with 10 neurons\n",
    "            nn = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000)\n",
    "            nn.fit(X_train, y_train)\n",
    "\n",
    "            # Predict\n",
    "            y_pred = nn.predict(X_test)\n",
    "\n",
    "            # Calculate accuracy and append to the list\n",
    "            acc_fold = accuracy_score(y_test, y_pred)\n",
    "            acc_ti.append(acc_fold)\n",
    "        \n",
    "        # Calculate the average accuracy across all folds\n",
    "        avg_acc_ti = np.mean(acc_ti)\n",
    "        #print(avg_acc_ti)\n",
    "        \n",
    "        # Print the accuracy scores for each fold and the average accuracy\n",
    "        #print(\"Accuracy scores for each fold:\", accuracy_scores)\n",
    "        #print(\"Average accuracy:\", average_accuracy)\n",
    "    \n",
    "        # Save average accuracy scores for a single time point\n",
    "        accuracy_early.append(avg_acc_ti)\n",
    "\n",
    "    # append single subject accuracy scores together\n",
    "    accuracy_early_all.append(accuracy_early)\n",
    "    \n",
    "    # Run the model for late block\n",
    "    for i in range(0, late_x.shape[1]):\n",
    "        #print(late_x[:,1,:].shape)\n",
    "        late_x_ti = late_x[:,i,:]\n",
    "        #print(late_x_ti.shape)\n",
    "    \n",
    "        X = late_x_ti\n",
    "        y = late_y\n",
    "        \n",
    "        # Create a KFold object with k=5\n",
    "        kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=None)\n",
    "        \n",
    "        # Initialize an empty list to store accuracy scores\n",
    "        acc_ti = []\n",
    "        \n",
    "        # Iterate through the folds\n",
    "        for train_index, test_index in kf.split(X,y):\n",
    "            # Split the data into training and testing sets\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "            # The network has one hidden layer with 10 neurons\n",
    "            nn = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000)\n",
    "            nn.fit(X_train, y_train)\n",
    "\n",
    "            # Predict\n",
    "            y_pred = nn.predict(X_test)\n",
    "\n",
    "            # Calculate accuracy and append to the list\n",
    "            acc_fold = accuracy_score(y_test, y_pred)\n",
    "            acc_ti.append(acc_fold)\n",
    "        \n",
    "        # Calculate the average accuracy across all folds\n",
    "        avg_acc_ti = np.mean(acc_ti)\n",
    "        #print(avg_acc_ti)\n",
    "        \n",
    "        # Print the accuracy scores for each fold and the average accuracy\n",
    "        #print(\"Accuracy scores for each fold:\", accuracy_scores)\n",
    "        #print(\"Average accuracy:\", average_accuracy)\n",
    "    \n",
    "        # Save average accuracy scores for a single time point\n",
    "        accuracy_late.append(avg_acc_ti)\n",
    "    \n",
    "\n",
    "    # append single subject accuracy scores together\n",
    "    accuracy_late_all.append(accuracy_late)\n",
    "\n",
    "accuracy_early_all = np.array(accuracy_early_all)\n",
    "accuracy_late_all = np.array(accuracy_late_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
